{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def template_match(img_rc, template_rc, img, template):\n",
    "\n",
    "    templates = [template]\n",
    "\n",
    "    w, h = template.shape[::-1]\n",
    "\n",
    "    method = cv2.TM_CCORR_NORMED\n",
    "\n",
    "    for tmplte in templates:\n",
    "\n",
    "        # Apply template Matching\n",
    "        res = cv2.matchTemplate(img,tmplte,method)\n",
    "        min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(res)\n",
    "\n",
    "        # If the method is TM_SQDIFF or TM_SQDIFF_NORMED, take minimum\n",
    "        if method in [cv2.TM_SQDIFF, cv2.TM_SQDIFF_NORMED]:\n",
    "            top_left = min_loc\n",
    "        else:\n",
    "            top_left = max_loc\n",
    "        bottom_right = (top_left[0] + w, top_left[1] + h)\n",
    "\n",
    "        img_rect = cv2.rectangle(img,top_left, bottom_right, 255, 2)\n",
    "        #hand_xray = img[y:y+h,x:x+w]\n",
    "\n",
    "        # grab cut hand using template\n",
    "        ret, thresh = cv2.threshold(img, 127, 255, 0)\n",
    "        im20, contours, hierarchy = cv2.findContours(thresh, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_TC89_KCOS)\n",
    "        cnt = len(contours)\n",
    "        print(cnt)\n",
    "\n",
    "        # Get the maximum area of interest\n",
    "        areas = [cv2.contourArea(c) for c in contours]\n",
    "        max_index = np.argmax(areas)\n",
    "        cnt_max=contours[max_index]\n",
    "\n",
    "        xg,yg,wg,hg = cv2.boundingRect(cnt_max)\n",
    "        rct = cv2.rectangle(img_rc,(xg,yg),(xg+wg,yg+hg),(255,255,255),2)\n",
    "        imgg = img_rc[yg:yg+hg,xg:xg+wg]\n",
    "\n",
    "        # end grab cut \n",
    "\n",
    "        plt.figure(figsize=(20,20))\n",
    "        plt.subplot(121),plt.imshow(img_rc)\n",
    "        plt.title('ROI'), plt.xticks([]), plt.yticks([])\n",
    "        plt.subplot(122),plt.imshow(imgg, cmap = 'gray')\n",
    "        plt.show()\n",
    "        plt.title('Matching Result'), plt.xticks([]), plt.yticks([])\n",
    "        plt.subplot(121),plt.imshow(res, cmap = 'gray')\n",
    "        plt.title('Detected Point'), plt.xticks([]), plt.yticks([])\n",
    "        plt.subplot(122),plt.imshow(template_rc)\n",
    "        plt.title('Template'), plt.xticks([]), plt.yticks([])\n",
    "        plt.suptitle(method)\n",
    "\n",
    "        plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:imageRecognition]",
   "language": "python",
   "name": "conda-env-imageRecognition-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
